{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed scraping - mutiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "from urllib.request import urlopen, urljoin\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain_url = 'https://www.yelp.com'\n",
    "base_url = 'https://www.yelp.com/search?find_loc=Fremont,+CA&start=10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_yelp_city_names():\n",
    "    cities_page_url = domain_url + '/city'\n",
    "    city_names = []\n",
    "\n",
    "    city_page_html = urlopen(cities_page_url).read().decode('utf-8')\n",
    "    \n",
    "    soup = BeautifulSoup(city_page_html, features='lxml')\n",
    "    city_href_links = soup.find_all('a', {'href': re.compile('/city/.*?')})\n",
    "    for link in city_href_links:\n",
    "        city_names.append(link.get_text())\n",
    "\n",
    "    return city_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_yelp_city_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_biz_urls(city_name, page_number):\n",
    "    base_url_template = 'https://www.yelp.com/search?find_loc=%s&start=%d'\n",
    "    biz_urls = []\n",
    "\n",
    "    target_city_url = base_url_template % (city_name.replace(' ', '+'), page_number)\n",
    "#     print(target_city_url)\n",
    "\n",
    "    target_city_page_html = urlopen(target_city_url).read().decode('utf-8')\n",
    "    if not target_city_page_html:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(target_city_page_html, features='lxml')\n",
    "\n",
    "    biz_href_elems = soup.find_all('a', {\n",
    "        'class': 'biz-name',\n",
    "        'href': re.compile('/biz/.*?')\n",
    "    })\n",
    "\n",
    "    for biz_href_elem in biz_href_elems:\n",
    "        biz_urls.append(biz_href_elem['href'])\n",
    "\n",
    "    return biz_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_biz_urls('Madison', 1)\n",
    "# target_city_page_html = urlopen('https://www.yelp.com/search?find_loc=Madison&start=1').read().decode('utf-8')\n",
    "# target_city_page_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    response = urlopen(url)\n",
    "    time.sleep(0.1)  # slightly delay for downloading\n",
    "    return response.read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(html):\n",
    "    biz_info = {}\n",
    "    \n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "\n",
    "    biz_name = soup.find('h1', {'class': 'biz-page-title'}).get_text().strip()\n",
    "    biz_info['biz_name'] = biz_name\n",
    "\n",
    "    review_count = soup.find('span', {'class': 'review-count rating-qualifier'}).get_text()\n",
    "    review_count = int(review_count.strip().replace(' reviews', ''))\n",
    "    biz_info['review_count'] = review_count\n",
    "\n",
    "    tags_elems = soup.find('span', {'class': 'category-str-list'})\\\n",
    "               .find_all('a')\n",
    "    tags = [tag_elem.get_text() for tag_elem in tags_elems]\n",
    "    biz_info['tags'] = tags\n",
    "\n",
    "    address_elem = soup.find('address')\n",
    "    biz_info['address_street'] = address_elem.find('span', {'itemprop': 'streetAddress'}).get_text()\n",
    "    biz_info['address_state'] = address_elem.find('span', {'itemprop': 'addressRegion'}).get_text()\n",
    "    biz_info['address_postalcode'] = address_elem.find('span', {'itemprop': 'postalCode'}).get_text()\n",
    "\n",
    "\n",
    "    biz_phone = soup.find('span', {'class': 'biz-phone'}).get_text().strip()\n",
    "    biz_phone = biz_phone.replace('(', '')\\\n",
    "                         .replace(')', '')\\\n",
    "                         .replace('-', '')\\\n",
    "                         .replace(' ', '')\n",
    "    biz_info['biz_phone'] = biz_phone\n",
    "\n",
    "    biz_info['more_biz_info'] = {}\n",
    "    more_biz_info_elems = soup.find('div', {'class': 'short-def-list'}).find_all('dl')\n",
    "    for biz_info_elem in more_biz_info_elems:\n",
    "        key = trim(biz_info_elem.find('dt').get_text())\n",
    "        value = trim(biz_info_elem.find('dd').get_text())\n",
    "        biz_info['more_biz_info'][key] = value\n",
    "\n",
    "    return biz_info\n",
    "\n",
    "def trim(string):\n",
    "    if not string:\n",
    "        return None\n",
    "    \n",
    "    string = string.strip()\\\n",
    "                   .replace('\\r', '')\\\n",
    "                   .replace('\\r', '')\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(biz_info):\n",
    "    print('[PROCESSED] ', biz_info['biz_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/biz/babalu-tapas-and-tacos-birmingham-3\n",
      "/biz/urban-standard-birmingham\n",
      "/biz/below-the-radar-huntsville\n",
      "/biz/chez-fonfon-birmingham\n",
      "/biz/saws-soul-kitchen-birmingham-3\n",
      "/biz/connors-steak-and-seafood-huntsville\n",
      "/biz/dolce-pan-bakery-huntsville\n",
      "/biz/trattoria-centrale-birmingham-2\n",
      "/biz/taqueria-el-cazador-huntsville-6\n",
      "/biz/toybox-bistro-huntsville\n",
      "/biz/hattie-bs-hot-chicken-bham-birmingham\n",
      "/biz/bamboo-on-2nd-birmingham\n",
      "/biz/the-poboy-factory-huntsville\n",
      "/biz/viet-huong-vietnamese-restaurant-huntsville\n",
      "/biz/carrigans-public-house-birmingham\n",
      "/biz/yo-mamas-birmingham-2\n",
      "/biz/betty-maes-restaurant-huntsville\n",
      "/biz/highlands-bar-and-grill-birmingham\n",
      "/biz/alchemy-lounge-huntsville-3\n",
      "/biz/cotton-row-restaurant-huntsville\n"
     ]
    }
   ],
   "source": [
    "urls = set()\n",
    "urls_size = 10\n",
    "for city_name in get_yelp_city_names():\n",
    "    urls = urls.union(get_biz_urls(city_name, 1))\n",
    "    if len(urls) > urls_size:\n",
    "        break\n",
    "    \n",
    "print('\\n'.join(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using multiprocessing to crawl the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributed crawling...\n",
      "[PROCESSED]  Hattie B’s Hot Chicken - Bham\n",
      "[PROCESSED]  Bamboo on 2nd\n",
      "[PROCESSED]  Toybox Bistro\n",
      "[PROCESSED]  Taqueria El Cazador\n",
      "[PROCESSED]  Trattoria Centrale\n",
      "[PROCESSED]  The Po’Boy Factory\n",
      "[PROCESSED]  Viet Huong Vietnamese Restaurant\n",
      "[Done]\n",
      "Distributed crawling...\n",
      "[PROCESSED]  Taqueria El Cazador\n",
      "[PROCESSED]  Toybox Bistro\n",
      "[PROCESSED]  Trattoria Centrale\n",
      "[Done]\n",
      "Distributed crawling...\n",
      "[PROCESSED]  Trattoria Centrale\n",
      "[PROCESSED]  Taqueria El Cazador\n",
      "[Done]\n",
      "Distributed crawling...\n",
      "[PROCESSED]  Trattoria Centrale\n",
      "[Done]\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(5)\n",
    "\n",
    "urls = list(urls)\n",
    "while urls:\n",
    "    print('Distributed crawling...')\n",
    "    \n",
    "    crawl_jobs = []\n",
    "    for url in urls:\n",
    "        crawl_jobs.append(pool.apply_async(crawl, args=(domain_url + url, )))\n",
    "        urls.pop()\n",
    "    htmls = [j.get() for j in crawl_jobs]\n",
    "    \n",
    "    parse_jobs = []\n",
    "    for html in htmls:\n",
    "        parse_jobs.append(pool.apply_async(parse, args=(html, )))\n",
    "    biz_infos = [j.get() for j in parse_jobs]\n",
    "    \n",
    "    process_jobs = [pool.apply_async(process, args=(biz_info,)) for biz_info in biz_infos]\n",
    "    [j.get() for j in process_jobs]\n",
    "    \n",
    "    print('[Done]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
