{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed scraping - mutiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "from urllib.request import urlopen, urljoin\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_url = 'https://www.yelp.com'\n",
    "base_url = 'https://www.yelp.com/search?find_loc=Fremont,+CA&start=10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yelp_city_names():\n",
    "    cities_page_url = domain_url + '/city'\n",
    "    city_names = []\n",
    "\n",
    "    city_page_html = urlopen(cities_page_url).read().decode('utf-8')\n",
    "    \n",
    "    soup = BeautifulSoup(city_page_html, features='lxml')\n",
    "    city_href_links = soup.find_all('a', {'href': re.compile('/city/.*?')})\n",
    "    for link in city_href_links:\n",
    "        city_names.append(link.get_text())\n",
    "\n",
    "    return city_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "['Birmingham', 'Huntsville', 'Chandler', 'Gilbert', 'Glendale', 'Mesa', 'Phoenix', 'Scottsdale', 'Tempe', 'Tucson', 'Anaheim', 'Bakersfield', 'Berkeley', 'Chula Vista', 'Concord', 'Davis', 'Elk Grove', 'Escondido', 'Fairfield', 'Fullerton', 'Hollywood', 'Lancaster', 'Los Angeles', 'Modesto', 'Moreno Valley', 'Oakland', 'Oceanside', 'Pasadena', 'Rancho Cucamonga', 'Richmond', 'Riverside', 'Roseville', 'Sacramento', 'San Bernardino', 'San Diego', 'San Francisco', 'San Jose', 'Santa Ana', 'Santa Clarita', 'Simi Valley', 'Sunnyvale', 'Torrance', 'West Covina', 'Arvada', 'Aurora', 'Boulder', 'Colorado Springs', 'Denver', 'Fort Collins', 'Westminster', 'Hartford', 'Stamford', 'Washington', 'Cape Coral', 'Clearwater', 'Coral Springs', 'Fort Lauderdale', 'Jacksonville', 'Lakeland', 'Miami', 'Orlando', 'Pembroke Pines', 'Pompano Beach', 'Saint Petersburg', 'Tallahassee', 'Tampa', 'West Palm Beach', 'Atlanta', 'Columbus', 'Sandy Springs', 'Savannah', 'Honolulu', 'Cedar Rapids', 'Des Moines', 'Iowa City', 'Boise', 'Aurora', 'Chicago', 'Springfield', 'Fort Wayne', 'Indianapolis', 'South Bend', 'Wichita', 'Lexington', 'Louisville', 'Baton Rouge', 'Lafayette', 'New Orleans', 'Shreveport', 'Boston', 'Cambridge', 'Worcester', 'Baltimore', 'Ann Arbor', 'Detroit', 'Lansing', 'Minneapolis', 'Rochester', 'Saint Paul', 'Kansas City', 'Saint Louis', 'Billings', 'Charlotte', 'Greensboro', 'Winston-Salem', 'Fargo', 'Omaha', 'Atlantic City', 'Jersey City', 'Newark', 'Albuquerque', 'Las Cruces', 'Santa Fe', 'Las Vegas', 'Reno', 'Albany', 'Brooklyn', 'Buffalo', 'Long Island', 'New York', 'Queens', 'Rochester', 'Syracuse', 'Cincinnati', 'Cleveland', 'Columbus', 'Toledo', 'Broken Arrow', 'Norman', 'Oklahoma City', 'Tulsa', 'Bend', 'Eugene', 'Portland', 'Salem', 'Philadelphia', 'Pittsburgh', 'Providence', 'Charleston', 'Columbia', 'Chattanooga', 'Knoxville', 'Memphis', 'Nashville', 'Arlington', 'Austin', 'Corpus Christi', 'Dallas', 'El Paso', 'Fort Worth', 'Houston', 'Laredo', 'Lubbock', 'Plano', 'San Antonio', 'Provo', 'Salt Lake City', 'Alexandria', 'Newport News', 'Norfolk', 'Virginia Beach', 'Bellevue', 'Bellingham', 'Seattle', 'Spokane', 'Tacoma', 'Vancouver', 'Green Bay', 'Madison', 'Milwaukee']\n"
     ]
    }
   ],
   "source": [
    "city_names = get_yelp_city_names()\n",
    "print(len(city_names))\n",
    "print(city_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biz_urls(city_name, page_number):\n",
    "    base_url_template = 'https://www.yelp.com/search?find_loc=%s&start=%d'\n",
    "    biz_urls = []\n",
    "\n",
    "    target_city_url = base_url_template % (city_name.replace(' ', '+'), page_number)\n",
    "#     print(target_city_url)\n",
    "\n",
    "    target_city_page_html = urlopen(target_city_url).read().decode('utf-8')\n",
    "    if not target_city_page_html:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(target_city_page_html, features='lxml')\n",
    "\n",
    "    biz_href_elems = soup.find_all('a', {\n",
    "        'class': 'biz-name',\n",
    "        'href': re.compile('/biz/.*?')\n",
    "    })\n",
    "\n",
    "    for biz_href_elem in biz_href_elems:\n",
    "        biz_urls.append(biz_href_elem['href'])\n",
    "\n",
    "    return biz_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/biz/graze-madison',\n",
       " '/biz/marigold-kitchen-madison',\n",
       " '/biz/pig-in-a-fur-coat-madison',\n",
       " '/biz/pauls-pelmeni-madison',\n",
       " '/biz/heritage-tavern-madison',\n",
       " '/biz/memorial-union-madison',\n",
       " '/biz/ha-long-bay-madison-2',\n",
       " '/biz/la-taguara-madison',\n",
       " '/biz/dotty-dumplings-dowry-madison',\n",
       " '/biz/pizza-brutta-madison']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_biz_urls('Madison', 1)\n",
    "# target_city_page_html = urlopen('https://www.yelp.com/search?find_loc=Madison&start=1').read().decode('utf-8')\n",
    "# target_city_page_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    response = urlopen(url)\n",
    "    time.sleep(0.1)  # slightly delay for downloading\n",
    "    return response.read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(html):\n",
    "    biz_info = {}\n",
    "    \n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "\n",
    "    biz_name = soup.find('h1', {'class': 'biz-page-title'}).get_text().strip()\n",
    "    biz_info['biz_name'] = biz_name\n",
    "\n",
    "    review_count = soup.find('span', {'class': 'review-count rating-qualifier'}).get_text()\n",
    "    review_count = int(review_count.strip().replace(' reviews', ''))\n",
    "    biz_info['review_count'] = review_count\n",
    "\n",
    "    tags_elems = soup.find('span', {'class': 'category-str-list'})\\\n",
    "               .find_all('a')\n",
    "    tags = [tag_elem.get_text() for tag_elem in tags_elems]\n",
    "    biz_info['tags'] = tags\n",
    "\n",
    "    address_elem = soup.find('address')\n",
    "    biz_info['address_street'] = address_elem.find('span', {'itemprop': 'streetAddress'}).get_text()\n",
    "    biz_info['address_state'] = address_elem.find('span', {'itemprop': 'addressRegion'}).get_text()\n",
    "    biz_info['address_postalcode'] = address_elem.find('span', {'itemprop': 'postalCode'}).get_text()\n",
    "\n",
    "\n",
    "    biz_phone = soup.find('span', {'class': 'biz-phone'}).get_text().strip()\n",
    "    biz_phone = biz_phone.replace('(', '')\\\n",
    "                         .replace(')', '')\\\n",
    "                         .replace('-', '')\\\n",
    "                         .replace(' ', '')\n",
    "    biz_info['biz_phone'] = biz_phone\n",
    "\n",
    "    biz_info['more_biz_info'] = {}\n",
    "    more_biz_info_elems = soup.find('div', {'class': 'short-def-list'}).find_all('dl')\n",
    "    for biz_info_elem in more_biz_info_elems:\n",
    "        key = trim(biz_info_elem.find('dt').get_text())\n",
    "        value = trim(biz_info_elem.find('dd').get_text())\n",
    "        biz_info['more_biz_info'][key] = value\n",
    "\n",
    "    return biz_info\n",
    "\n",
    "def trim(string):\n",
    "    if not string:\n",
    "        return None\n",
    "    \n",
    "    string = string.strip()\\\n",
    "                   .replace('\\r', '')\\\n",
    "                   .replace('\\r', '')\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(biz_info):\n",
    "    print('[PROCESSED] ', biz_info['biz_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/biz/dolce-pan-bakery-huntsville\n",
      "/biz/bamboo-on-2nd-birmingham\n",
      "/biz/saws-soul-kitchen-birmingham-3\n",
      "/biz/the-poboy-factory-huntsville\n",
      "/biz/yo-mamas-birmingham-2\n",
      "/biz/toybox-bistro-huntsville\n",
      "/biz/betty-maes-restaurant-huntsville\n",
      "/biz/chez-fonfon-birmingham\n",
      "/biz/babalu-tapas-and-tacos-birmingham-3\n",
      "/biz/connors-steak-and-seafood-huntsville\n",
      "/biz/viet-huong-vietnamese-restaurant-huntsville\n",
      "/biz/highlands-bar-and-grill-birmingham\n",
      "/biz/cotton-row-restaurant-huntsville\n",
      "/biz/trattoria-centrale-birmingham-2\n",
      "/biz/hattie-bs-hot-chicken-bham-birmingham\n",
      "/biz/carrigans-public-house-birmingham\n",
      "/biz/taqueria-el-cazador-huntsville-6\n",
      "/biz/alchemy-lounge-huntsville-3\n",
      "/biz/below-the-radar-huntsville\n",
      "/biz/urban-standard-birmingham\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = set()\n",
    "urls_size = 10\n",
    "for city_name in get_yelp_city_names():\n",
    "    urls = urls.union(get_biz_urls(city_name, 1))\n",
    "    if len(urls) > urls_size:\n",
    "        break\n",
    "    \n",
    "print('\\n'.join(urls))\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using multiprocessing to crawl the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributed crawling...\n",
      "[PROCESSED]  Hattie B’s Hot Chicken - Bham\n",
      "[PROCESSED]  Bamboo on 2nd\n",
      "[PROCESSED]  Toybox Bistro\n",
      "[PROCESSED]  Taqueria El Cazador\n",
      "[PROCESSED]  Trattoria Centrale\n",
      "[PROCESSED]  The Po’Boy Factory\n",
      "[PROCESSED]  Viet Huong Vietnamese Restaurant\n",
      "[Done]\n",
      "Distributed crawling...\n",
      "[PROCESSED]  Taqueria El Cazador\n",
      "[PROCESSED]  Toybox Bistro\n",
      "[PROCESSED]  Trattoria Centrale\n",
      "[Done]\n",
      "Distributed crawling...\n",
      "[PROCESSED]  Trattoria Centrale\n",
      "[PROCESSED]  Taqueria El Cazador\n",
      "[Done]\n",
      "Distributed crawling...\n",
      "[PROCESSED]  Trattoria Centrale\n",
      "[Done]\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(5)\n",
    "\n",
    "urls = list(urls)\n",
    "while urls:\n",
    "    print('Distributed crawling...')\n",
    "    \n",
    "    crawl_jobs = []\n",
    "    for url in urls:\n",
    "        crawl_jobs.append(pool.apply_async(crawl, args=(domain_url + url, )))\n",
    "        urls.pop()\n",
    "    htmls = [j.get() for j in crawl_jobs]\n",
    "    \n",
    "    parse_jobs = []\n",
    "    for html in htmls:\n",
    "        parse_jobs.append(pool.apply_async(parse, args=(html, )))\n",
    "    biz_infos = [j.get() for j in parse_jobs]\n",
    "    \n",
    "    process_jobs = [pool.apply_async(process, args=(biz_info,)) for biz_info in biz_infos]\n",
    "    [j.get() for j in process_jobs]\n",
    "    \n",
    "    print('[Done]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
